import streamlit as st
import pandas as pd
import numpy as np
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist, squareform
import umap


# Custom K-Means Implementation
class CustomKMeans:
    def __init__(self, n_clusters=3, max_iters=100):
        self.n_clusters = n_clusters
        self.max_iters = max_iters
        self.centroids = None
        self.labels = None

    def _euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2) ** 2))

    def _init_centroids(self, X):
        # Random initialization of centroids
        indices = np.random.choice(len(X), self.n_clusters, replace=False)
        return X[indices]

    def fit(self, X):
        # Normalize data
        X = (X - X.mean(axis=0)) / X.std(axis=0)

        self.centroids = self._init_centroids(X)

        for _ in range(self.max_iters):
            # Assign clusters
            clusters = [[] for _ in range(self.n_clusters)]

            for point in X:
                distances = [self._euclidean_distance(point, centroid) for centroid in self.centroids]
                cluster_index = np.argmin(distances)
                clusters[cluster_index].append(point)

            # Store previous centroids
            prev_centroids = self.centroids.copy()

            # Update centroids
            self.centroids = np.array([np.mean(cluster, axis=0) if len(cluster) > 0 else prev_centroids[i]
                                       for i, cluster in enumerate(clusters)])

            # Check convergence
            if np.allclose(self.centroids, prev_centroids):
                break

        # Assign final labels
        self.labels = np.array([np.argmin([self._euclidean_distance(point, centroid)
                                           for centroid in self.centroids]) for point in X])

        return self


# Custom DBSCAN Implementation
class CustomDBSCAN:
    def __init__(self, eps=0.5, min_samples=5):
        self.eps = eps
        self.min_samples = min_samples
        self.labels = None

    def _euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2) ** 2))

    def fit(self, X):
        # Normalize data
        X = (X - X.mean(axis=0)) / X.std(axis=0)

        n_points = X.shape[0]
        self.labels = np.zeros(n_points, dtype=int)
        current_label = 0

        for point_idx in range(n_points):
            if self.labels[point_idx] != 0:
                continue

            # Find neighbors
            neighbors = [
                idx for idx in range(n_points)
                if self._euclidean_distance(X[point_idx], X[idx]) < self.eps
            ]

            if len(neighbors) < self.min_samples:
                self.labels[point_idx] = -1  # Noise point
            else:
                current_label += 1
                self.labels[point_idx] = current_label

                # Expand cluster
                seed_set = set(neighbors)
                while seed_set:
                    current_point = seed_set.pop()

                    if self.labels[current_point] == -1:
                        self.labels[current_point] = current_label

                    if self.labels[current_point] != 0:
                        continue

                    self.labels[current_point] = current_label

                    current_neighbors = [
                        idx for idx in range(n_points)
                        if self._euclidean_distance(X[current_point], X[idx]) < self.eps
                    ]

                    if len(current_neighbors) >= self.min_samples:
                        seed_set.update(current_neighbors)

        return self


def perform_apriori_analysis(transactions):
    # Konwersja zestaw贸w produkt贸w do listy list, aby unikn problem贸w z serializacj
    transactions = [list(map(str, transaction)) for transaction in transactions]

    # U偶ycie TransactionEncoder
    te = TransactionEncoder()
    te_ary = te.fit(transactions).transform(transactions)
    df = pd.DataFrame(te_ary, columns=te.columns_)

    # Znajd藕 czste zbiory (itemsets)
    frequent_itemsets = apriori(df, min_support=0.02, use_colnames=True)

    # Zabezpieczenie przed bdem num_itemsets
    try:
        rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
    except TypeError:
        rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1,
                                  num_itemsets=len(frequent_itemsets))

    # Konwersja frozenset na zwyke zbiory, aby unikn problem贸w z serializacj
    frequent_itemsets['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: set(x))
    rules['antecedents'] = rules['antecedents'].apply(lambda x: set(x))
    rules['consequents'] = rules['consequents'].apply(lambda x: set(x))

    return frequent_itemsets, rules


def optimize_spectral_clustering(X, n_clusters=3):
    # U偶ycie UMAP do redukcji wymiarowoci przed klastrowaniem
    reducer = umap.UMAP(n_components=5, random_state=42)
    X_reduced = reducer.fit_transform(X)

    from sklearn.cluster import SpectralClustering
    from sklearn.preprocessing import StandardScaler

    # Standaryzacja danych
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_reduced)

    # Spektralne grupowanie na zredukowanych danych
    spectral = SpectralClustering(
        n_clusters=n_clusters,
        affinity='nearest_neighbors',
        random_state=42,
        n_neighbors=10
    )
    return spectral.fit_predict(X_scaled)


def main():
    st.title(" Wielowymiarowa Analiza Danych Sprzeda偶owych")

    # Sidebar
    st.sidebar.header("Opcje Analizy")
    uploaded_file = st.sidebar.file_uploader("Wgraj plik CSV", type=['csv'])

    if uploaded_file is not None:
        # Load data
        df = pd.read_csv(uploaded_file)
        st.write("### Wczytane dane:")
        st.dataframe(df.head())

        # Prepare product transactions for Apriori
        transactions = df.groupby('Transaction_ID')['Product'].apply(list).tolist()

        # Analysis Tabs
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "Analiza Asocjacji",
            "Grupowanie K-Means",
            "Grupowanie DBSCAN",
            "Grupowanie Hierarchiczne",
            "Pozostae Metody Grupowania"
        ])

        with tab1:
            st.header("Analiza Asocjacji - Algorytm Apriori")
            if st.button("Przeprowad藕 Analiz Asocjacji"):
                frequent_itemsets, rules = perform_apriori_analysis(transactions)

                st.subheader("Czste Zestawy Produkt贸w")
                st.write("""
                ### Interpretacja Czstych Zestaw贸w Produkt贸w
                - **Support (Wsparcie)**: Procent wszystkich transakcji, kt贸re zawieraj dany zestaw produkt贸w
                - **Im wy偶sza warto support, tym czciej produkty s kupowane razem**
                - Przykad: Support 0.05 oznacza, 偶e 5% wszystkich transakcji zawiera ten zestaw produkt贸w
                """)
                st.dataframe(frequent_itemsets.sort_values(by='support', ascending=False).head(10))

                st.subheader("Reguy Asocjacyjne")
                st.write("""
                ### Interpretacja Regu Asocjacyjnych
                - **Antecedents**: Produkty inicjujce regu
                - **Consequents**: Produkty, kt贸re s kupowane razem z antecedents
                - **Lift**: Miara siy zwizku midzy produktami
                  - Lift > 1: Produkty kupowane czciej razem ni偶 losowo
                  - Lift = 1: Produkty kupowane niezale偶nie
                  - Lift < 1: Produkty rzadziej kupowane razem
                """)
                st.dataframe(rules.sort_values(by='lift', ascending=False).head(10))

        with tab2:
            st.header("Grupowanie K-Means")
            n_clusters = st.slider("Liczba klastr贸w", 2, 10, 3)

            # Prepare data for clustering
            features_for_clustering = ['Quantity', 'Price_Per_Unit', 'Total_Price']
            X = df[features_for_clustering].values

            kmeans = CustomKMeans(n_clusters=n_clusters)
            kmeans.fit(X)

            # Visualize clusters
            plt.figure(figsize=(10, 6))
            scatter = plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels, cmap='viridis')
            plt.title('K-Means Clustering')
            plt.xlabel('Quantity')
            plt.ylabel('Price Per Unit')
            plt.colorbar(scatter, label='Klaster')
            st.pyplot(plt)

            st.write("""
            ### Interpretacja Grupowania K-Means
            - Ka偶dy kolor reprezentuje osobny klaster (grup) produkt贸w
            - Produkty w tym samym kolorze s do siebie podobne pod wzgldem iloci i ceny
            - Algorytm dzieli dane na K r贸wnomiernych, kulistych klastr贸w
            - Centroid (rodek klastra) reprezentuje typowy punkt dla danej grupy

            **Jak czyta wykres**:
            - O X: Ilo produkt贸w
            - O Y: Cena produktu
            - Kolor: Przynale偶no do klastra
            """)

        with tab3:
            st.header("Grupowanie DBSCAN")
            eps = st.slider("Epsilon", 0.1, 2.0, 0.5)
            min_samples = st.slider("Minimalna liczba pr贸bek", 2, 20, 5)

            X = df[['Quantity', 'Price_Per_Unit', 'Total_Price']].values

            dbscan = CustomDBSCAN(eps=eps, min_samples=min_samples)
            dbscan.fit(X)

            plt.figure(figsize=(10, 6))
            scatter = plt.scatter(X[:, 0], X[:, 1], c=dbscan.labels, cmap='viridis')
            plt.title('DBSCAN Clustering')
            plt.xlabel('Quantity')
            plt.ylabel('Price Per Unit')
            plt.colorbar(scatter, label='Klaster')
            st.pyplot(plt)

            st.write("""
            ### Interpretacja Grupowania DBSCAN
            - Algorytm grupuje punkty na podstawie gstoci
            - Czarny kolor: Punkty odbiegajce od normy (szum)
            - R贸偶ne kolory: Odrbne, gste skupiska danych

            **Kluczowe parametry**:
            - Epsilon (eps): Maksymalna odlego midzy punktami w klastrze
            - Min Samples: Minimalna liczba punkt贸w wymagana do utworzenia klastra

            **Jak czyta wykres**:
            - Punkty blisko siebie w gstym obszarze nale偶 do tego samego klastra
            - Punkty oddalone s oznaczane jako szum
            """)

        with tab4:
            st.header("Grupowanie Hierarchiczne")
            # Compute linkage matrix
            X = df[['Quantity', 'Price_Per_Unit', 'Total_Price']].values
            Z = linkage(X, method='ward')

            plt.figure(figsize=(10, 6))
            dendrogram(Z, truncate_mode='lastp', p=30, leaf_rotation=90)
            plt.title('Dendrogram Grupowania Hierarchicznego')
            plt.xlabel('Pr贸bki')
            plt.ylabel('Odlego')
            st.pyplot(plt)

            st.write("""
            ### Interpretacja Dendrogramu
            - Dendrogram to diagram przypominajcy drzewo genealogiczne danych
            - Pionowa o (Y) pokazuje odlego/podobiestwo midzy grupami
            - Im ni偶ej si cz gazie, tym wiksze podobiestwo midzy grupami

            **Jak czyta**:
            - Najni偶sze poczenia reprezentuj najbardziej podobne grupy produkt贸w
            - Wysoko pionowych linii wskazuje na r贸偶nice midzy klastrami
            - Mo偶esz "uci" drzewo na r贸偶nych wysokociach, aby otrzyma r贸偶n liczb klastr贸w
            """)

        with tab5:
            st.header("Pozostae Metody Grupowania")
            from sklearn.mixture import GaussianMixture

            X = df[['Quantity', 'Price_Per_Unit', 'Total_Price']].values

            st.subheader("Gaussian Mixture Models")
            gmm = GaussianMixture(n_components=3)
            gmm_labels = gmm.fit_predict(X)

            plt.figure(figsize=(10, 6))
            scatter = plt.scatter(X[:, 0], X[:, 1], c=gmm_labels, cmap='viridis')
            plt.title('Gaussian Mixture Models Clustering')
            plt.xlabel('Quantity')
            plt.ylabel('Price Per Unit')
            plt.colorbar(scatter, label='Klaster')
            st.pyplot(plt)

            st.write("""
            ### Interpretacja Gaussian Mixture Models
            - Zakada, 偶e dane pochodz z mieszaniny rozkad贸w gaussowskich
            - Ka偶dy klaster reprezentuje osobny rozkad prawdopodobiestwa
            - Algorytm przypisuje prawdopodobiestwo przynale偶noci do klastra

            **Charakterystyka**:
            - Elastyczny w grupowaniu danych o r贸偶nych ksztatach
            - Uwzgldnia niepewno przynale偶noci do klastra
            - Ka偶dy punkt mo偶e mie czciow przynale偶no do kilku klastr贸w
            """)

            st.subheader("Spectral Clustering")
            spectral_labels = optimize_spectral_clustering(X)

            plt.figure(figsize=(10, 6))
            scatter = plt.scatter(X[:, 0], X[:, 1], c=spectral_labels, cmap='viridis')
            plt.title('Spectral Clustering')
            plt.xlabel('Quantity')
            plt.ylabel('Price Per Unit')
            plt.colorbar(scatter, label='Klaster')
            st.pyplot(plt)

            st.write("""
            ### Interpretacja Spectral Clustering
            - Grupowanie oparte na teorii spektralnej graf贸w
            - Przeksztaca dane do przestrzeni o ni偶szej wymiarowoci przed grupowaniem
            - Efektywny dla danych o skomplikowanej, nieliniowej strukturze

            **Kluczowe cechy**:
            - Radzi sobie z grupami o nieregularnych ksztatach
            - Wykorzystuje wasnoci macierzy podobiestwa
            - Dziaa dobrze, gdy klastra nie s kulistymi skupiskami
            """)


if __name__ == "__main__":
    st.set_page_config(page_title="Analiza Sprzeda偶y", page_icon="")
    main()